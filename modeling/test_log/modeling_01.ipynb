{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. ê¸°ë³¸í™˜ê²½ ì„¸íŒ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu í™˜ê²½ ì²´í¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3050 Ti Laptop GPU (UUID: GPU-b5b4dfdb-dd9f-1ff3-bc8b-d74672661a90)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# output : \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ë°ì´í„°ì…‹ê³¼ transformers ì„¤ì¹˜\n",
    "# %pip install datasets>=2.6.1\n",
    "# %pip install git+https://github.com/huggingface/transformers\n",
    "\n",
    "# # ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ librosa ì„¤ì¹˜\n",
    "# %pip install librosa\n",
    "\n",
    "# # ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ evaluateì™€ jiwer ì„¤ì¹˜\n",
    "# %pip install evaluate>=0.30\n",
    "# %pip install jiwer\n",
    "\n",
    "# # ì¸í„°ë™í‹°ë¸Œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ gradio ì„¤ì¹˜\n",
    "# %pip install gradio\n",
    "\n",
    "# # Transformersì™€ PyTorchë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ê¸° ìœ„í•œ accelerate ì„¤ì¹˜\n",
    "# %pip install transformers[torch]\n",
    "# %pip install accelerate>=0.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Use Data Collator to perform Speech Seq2Seq with padding\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juneh\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ìŒì„± ì²˜ë¦¬ì— í•„ìš”í•œ ê¸°ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "# Load Feature extractor: WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Load Tokenizer: WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"english\", task=\"transcribe\")\n",
    "\n",
    "# Load Processor: WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"english\", task=\"transcribe\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the combined 'train' dataset\n",
    "loaded_train_dataset = load_from_disk('./data/map_dataset/train')\n",
    "\n",
    "# Load the combined 'test' dataset\n",
    "loaded_test_dataset = load_from_disk('./data/map_dataset/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_features', 'labels'],\n",
      "    num_rows: 9523\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_features', 'labels'],\n",
      "    num_rows: 502\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(loaded_train_dataset)\n",
    "print(loaded_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. í•™ìŠµ ë° í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "  6%|â–‹         | 63/1000 [4:30:01<67:03:19, 257.63s/it]"
     ]
    }
   ],
   "source": [
    "# STEP 5.1. Initialize the Data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# STEP 5.1. Define evaluation metric\n",
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "# STEP 5.3. Load a pre-trained Checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "     \n",
    "\n",
    "# STEP 5.4. Define the training configuration\n",
    "\"\"\"\n",
    "Check for Seq2SeqTrainingArguments here:\n",
    "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n",
    "\"\"\"\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper_base_0401\",  # ì €ì¥ëœ ëª¨ë¸ ë° ê²°ê³¼ë¬¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "    per_device_train_batch_size=32,  # í•œ ë²ˆì— ì²˜ë¦¬ë˜ëŠ” í›ˆë ¨ ë°°ì¹˜ í¬ê¸°\n",
    "    gradient_accumulation_steps=2,  # ë°°ì¹˜ í¬ê¸° ê°ì†Œì‹œ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ í†µí•œ í•™ìŠµ ì•ˆì •í™”\n",
    "    learning_rate=1e-5,  # í•™ìŠµë¥ \n",
    "    warmup_steps=100,  # ì´ˆê¸° í•™ìŠµë¥  ì¡°ì •ì„ ìœ„í•œ ì›œì—… ìŠ¤í… ìˆ˜ / ì¼ë°˜ì ìœ¼ë¡œëŠ” 10% ~ 20%ì˜ ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ì‹œë„\n",
    "    max_steps=1000,  # ì „ì²´ í›ˆë ¨ ìŠ¤í… ìˆ˜\n",
    "    gradient_checkpointing=True,  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì„ í†µí•œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    # fp16=True,  # FP16 í˜•ì‹ìœ¼ë¡œ í›ˆë ¨ ìˆ˜í–‰ (ë°˜ì •ë°€ë„ ë¶€ë™ì†Œìˆ˜ì )( cpu ê°€ë™ì‹œ ì•ˆì”€)\n",
    "    evaluation_strategy=\"no\",  # ê²€ì¦ ìˆ˜í–‰ ì „ëµ ì„¤ì •\n",
    "    per_device_eval_batch_size=16,  # í•œ ë²ˆì— ì²˜ë¦¬ë˜ëŠ” ê²€ì¦ ë°°ì¹˜ í¬ê¸°\n",
    "    predict_with_generate=True,  # ìƒì„±ëœ í† í°ì„ í†µí•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    generation_max_length=225,  # ìƒì„±ëœ í† í°ì˜ ìµœëŒ€ ê¸¸ì´ (225ìœ ì§€)\n",
    "    eval_steps=100,  # ê²€ì¦ ìˆ˜í–‰ ìŠ¤í… ìˆ˜\n",
    "    logging_steps=100,  # ë¡œê·¸ ê¸°ë¡ ìŠ¤í… ìˆ˜\n",
    "    load_best_model_at_end=False,  # í›ˆë ¨ ì¢…ë£Œ ì‹œ ìµœì  ëª¨ë¸ ë¡œë“œ ì—¬ë¶€\n",
    "    metric_for_best_model=\"wer\",  # ìµœì  ëª¨ë¸ ì„ ì •ì„ ìœ„í•œ í‰ê°€ ì§€í‘œ \n",
    "    greater_is_better=False,  # í‰ê°€ ì§€í‘œ ê°’ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ì€ì§€ ì—¬ë¶€\n",
    "    save_steps=200  # ë³€ê²½ëœ save_steps ê°’\n",
    ")\n",
    "\n",
    "# Initialize a trainer.\n",
    "\"\"\"\n",
    "Forward the training arguments to the Hugging Face trainer along with our model,\n",
    "dataset, data collator and compute_metrics function.\n",
    "\"\"\"\n",
    "# ì§€ì •ëœ ì¸ì ë° êµ¬ì„±ìš”ì†Œë¡œ íŠ¸ë ˆì´ë„ˆë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,                   # ì´ì „ì— ì •ì˜í•œ í›ˆë ¨ ì¸ì\n",
    "    model=model,                          # í›ˆë ¨í•  ASR ëª¨ë¸\n",
    "    train_dataset=loaded_train_dataset,# í›ˆë ¨ ë°ì´í„°ì…‹\n",
    "    eval_dataset=loaded_test_dataset,  # í‰ê°€ ë°ì´í„°ì…‹\n",
    "    data_collator=data_collator,           # ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì½œë ˆì´í„°\n",
    "    compute_metrics=compute_metrics,          # wer ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    tokenizer=processor.feature_extractor, # ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í† í¬ë‚˜ì´ì €\n",
    ")\n",
    "\n",
    "# Save processor object before starting training\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# STEP 5.5. Training\n",
    "\"\"\"\n",
    "Training will take appr. 5-10 hours depending on your GPU.\n",
    "\"\"\"\n",
    "print('Training ì‹œì‘')\n",
    "trainer.train()   # <-- training ì‹œì‘\n",
    "print('Training ì™„ë£Œ')\n",
    "\n",
    "#\"Step\": ëª¨ë¸ì˜ í›ˆë ¨ ê³¼ì •ì—ì„œ ì§„í–‰ë˜ëŠ” ê° ìŠ¤í…ì„ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ìì…ë‹ˆë‹¤.\n",
    "#ìŠ¤í…ì€ ì£¼ë¡œ ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ëª¨ë¸ì´ ì—…ë°ì´íŠ¸ë˜ëŠ” ì§€ì ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "# Training LossëŠ” ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ\n",
    "# Training Lossê°€ ê°ì†Œí•˜ë©´ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ ë” ì˜ í•™ìŠµí•˜ê³  ìˆëŠ” ê²ƒ\n",
    "# ëª¨ë¸ì´ ë°ì´í„°ì— ë” ì˜ ì í•©ë˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸\n",
    "\n",
    "# Validation LossëŠ” ëª¨ë¸ì´ ì´ì „ì— ë³¸ ì ì´ ì—†ëŠ” ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì •í™•ë„\n",
    "# í›ˆë ¨ ê³¼ì • ì¤‘ì— ì¼ì • ì£¼ê¸°ë§ˆë‹¤ ê²€ì¦ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ Validation Lossë¥¼ ê³„ì‚°\n",
    "# ì´ ê°’ì´ ê°ì†Œí•˜ë©´ ëª¨ë¸ì´ ì¼ë°˜í™”ë˜ê³  ìˆëŠ” ê²ƒì„ ì˜ë¯¸\n",
    "# ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ë¿ë§Œ ì•„ë‹ˆë¼ ìƒˆë¡œìš´ ë°ì´í„°ì—ë„ ì˜ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµë˜ê³  ìˆë‹¤ëŠ” ê²ƒ\n",
    "\n",
    "# \"CER\" (Character Error Rate): í›ˆë ¨ ì¤‘ì— ì¼ì • ì£¼ê¸°ë§ˆë‹¤ ê²€ì¦ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬\n",
    "#ëª¨ë¸ì˜ ë¬¸ì ì—ëŸ¬ ë¹„ìœ¨(CER)ì„ í‰ê°€í•œ ê°’ì…ë‹ˆë‹¤.\n",
    "#CERì€ í…ìŠ¤íŠ¸ ë¶„ì•¼ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‰ê°€ ì§€í‘œ ì¤‘ í•˜ë‚˜ë¡œ,\n",
    "#ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ì™€ ì‹¤ì œ í…ìŠ¤íŠ¸ ì‚¬ì´ì˜ ë¬¸ì ìˆ˜ì¤€ ì˜¤ë¥˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "#CERì´ ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµëœ ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.save_model(\"./model/whisper_base_0331\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/whisper_base_0824_ver1\")\n",
    "\n",
    "# Specify the directory where you want to save the tokenizer\n",
    "save_directory = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
    "\n",
    "# Save the tokenizer to the specified directory\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ëª¨ë¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import soundfile as sf  # soundfile ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
    "\n",
    "# ASR íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
    "model_name_or_path = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
    "asr = pipeline(model=model_name_or_path, task=\"automatic-speech-recognition\")\n",
    "\n",
    "# ìŒì„± íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
    "audio_file_paths = [\n",
    "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02601769.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02743434.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02989139.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D1-E-K0KK-03006747.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D1-E-K0KK-03017978.wav\",\n",
    "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D4-E-K0KK-02872759.wav\",\n",
    "    # ì¶”ê°€ ìŒì„± íŒŒì¼ ê²½ë¡œ\n",
    "]\n",
    "\n",
    "# ASR í•¨ìˆ˜ ì •ì˜\n",
    "def transcribe_audio(audio_path):\n",
    "    transcription = asr(audio_path)\n",
    "    return transcription['text']  # Use 'text' key to get the transcribed text\n",
    "\n",
    "# ê° ìŒì„± íŒŒì¼ì— ëŒ€í•œ ì²˜ë¦¬ ë° ì¶œë ¥\n",
    "for audio_file_path in audio_file_paths:\n",
    "    transcription_text = transcribe_audio(audio_file_path)\n",
    "    print(f\"ìŒì„± íŒŒì¼: {audio_file_path}\")\n",
    "    print(\"í…ìŠ¤íŠ¸ ì¶œë ¥:\", transcription_text)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
